---
layout: post
title:  "astro-ph Reading in 2020"
date:   2020-08-04 22:00:00
author: Jiaxuan Li
categories: paper
---
Again, feel very ashamed for the decadence in the past year 2019. I will try to read as many paper as possible and do my best in 2020.

因为新冠和毕业季，我近半年没有认真读文献了。终于在北京安顿了下来，重新捡起阅读的习惯，记录在这里。

[AAS-Nova](https://aasnova.org)

[Astrobite](https://astrobites.org)

## September 2020

### [CS 231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/index.html)

- [Google cloud setup tutorial](https://github.com/cs231n/gcloud)

- Lecture2: 

  - Linear Classifier only learns one template for one category (the row of the weight matrix). 

- Lecture3:

  - If the image changes a little bit, SVM loss (**hinge loss**) will not change. 
  - You can design your loss function based on how bad you think negative classifications are (linear hinge or squar ed hinge loss). 
  - Add an additional term in the loss function as **regularization**. L1 regularization prefers sparse solutions. 
  - Softmax loss ([**cross-entropy loss**](https://www.youtube.com/watch?v=ErfnhcEV1O8)):  - log probability = - log (normalize(exp(score_of_true_class))). Softmax loss is sensitive to a little change in the input image (thus the score). Actually `softmax` is a squashing function that turns scores into probability, then cross-entropy loss is used.

{% include image.html url="/images/astro-ph/CS231n-Lec3-1.png" caption="Loss and regularization" width=400 align="center" %}

  - In DL, you always use analytic gradient (instead of numeric gradient).
  - Getting learning rate correct is the first step in tuning hyper-parameters. 
  - Vanilla gradient descent, stochastic gradient descent (SGD), ADAM
  - If we take the whole training set to calculate the loss, gradient, and update parameters, it will take a very long time to iterate over all images. We use stochastic gradient descent (SGD) and **minibatch**. For each time, we sample a small subset of the training set (called minibatch), and update the parameters based on these images. 
  - **Feature representation**: 
    - Extract some features of your images, then flatten features into a 1-D array, and feed it into linear classifier. 
    - Histogram of Oriented Gradients (HoG): detecting edge is critical to human vision system. 
    - "Visual words"
    - Classical image classification methods first extract features and concatenate features together and feed them to a linear classifier. The feature extraction module will not get updated during training. NN is different, it gets updated during training. 

- Lecture4:

  - Sigmoid: $\frac{1}{1+e^{-x}}$
  - The gradient with respect to a variable should have the same shape as the variable (vectorized operations)
  - Each row of the weight matrix is like a map: where to look for features 
  - more neurons = more capacity
  - Actually three FC layers are just like $f = W_3 \cdot \max(W_2 \cdot \max(W_1\cdot x))$. The notion of "layer" allows us to use efficient vectorized code (matrix multiplies). 

- Lecture5:

  - CNN can also be used for object detection and segmentation.

  - In CNN, each filter looks for one type of feature. We use multiple filters/kernels. 

  - Filters at the earlier/later layers usually represent low-level/high lever feature. Each filter does convolution in 3 colors (rgb). The depth of output is the number of filters you have. 

  - **Output size = (N + 2 * padding - F) / stride + 1**

    The number of filters is often powers of 2: 32, 64, 128.
    
  - Pooling layer: make representations smaller and more manageable. We only do pooling spatially, not in the depth direction. Common one is **max pooling**. 
  

{% include image.html url="/images/astro-ph/CS231n-Lec4-CNN.png" caption="CNN output size formulae" width=400 align="center" %}



​    






## August 2020
### [A Short Introduction to Entropy, Cross-Entropy and KL-Divergence](https://www.youtube.com/watch?v=ErfnhcEV1O8)

- Entropy: $H(p) = -\sum_{i}p_i\log(p_i)$, the average information we get from the true distribution.
- Cross-entropy between two distributions (also a "distance" between two vectors): $H(p,q) = -\sum_i p_i \log(q_i)$, the average information we get from the assumed distirbution.
- Kullback-Leibler divergence: $D_{KL}(p||q) = H(p,q) - H(p) = \sum_i p_i \log\left(\frac{p_i}{q_i}\right)$.
- Softmax loss: to minimize the distance between the true probability distribution and the estimated distribution. For image classification task, the loss becomes $-\log q_{\text{true}}$.

### [PyTorch Performance Tuning Guide - Szymon Migacz, NVIDIA](https://www.youtube.com/watch?v=9mS1fIYj1So)

{% include image.html url="https://pbs.twimg.com/media/Ego_hTIUwAARnS6?format=png&name=900x900" caption="Optimizing PyTorch code" width=350 align="right" %}

### [The State of Machine Learning Frameworks in 2019](https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/)

- "Researchers are abandoning TensorFlow and flocking to PyTorch in droves. Meanwhile in industry, Tensorflow is currently the platform of choice, but that may not be true for long."
- PyTorch for researchers: simple, great API (pythonic), nice performance. 
- Industry is slower to adopt new technologies than researchers. Researches care about fast iteration, easy-to-use on their own machine. But industry cares about performance (1% faster means a lot to them), ability to transfer to mobile devices, etc. TensorFlow has solutions for all these issues. 

### [Deep Learning with Structured Data](https://livebook.manning.com/book/deep-learning-with-structured-data/chapter-1/v-6/6)

- Chap 1
  - Drawbacks of DL: large data volume (slow); non-transparent algorithm (well-known); tuning lots of things (overfitting, exploding gradients, and other hyperparameters).
  - Solutions: transfer learning (use a well-trained model as the startpoint for another task); use commercial cloud platforms
  - Structured data (tabular): relational database (**rows and columns** in a table), SQL. Unstructured data: image, video, audio, also text from HTML/XML/JSON. Many of the celebrated applications of deep learning have been with unstructured data such as images, audio, and text. 



### [Impact of Satellite Constellation on Optical Astronomy and Recommendations towards Mitigations](https://aas.org/satellite-constellations-1-workshop-report)

- Low-Earth Orbit satellites (LEOsats, 600-1000 km above the ground) will fundamentally change the optical and NIR observations planned in the following decades, especially wide-field imaging projects.
- LEOsats reduces the SNR of imaging, cause missed objects, introduces systematic uncertainties in large sample studies. Hence has impact on finding near-earth objects, GW counterpart, etc.
- Satellites below 600 km are visible around twilights, thus have heavy impact on twilight observers. High altitude satellites are visible all nights long (since it's easier to reflect sunlights at high altitude). They impact a larger set of astronomical programs. 
- Solutions:
  - Don't launch LEOsats constellation any more. Or launch satellites that are clumped as tight as possible. 
  - Make their orbits below 600 km, and control orbits so that they reflect less lights to us.
  - Make them dark to decrease the albedo.
  - Avoid satellite trails making use of their orbits when conducting observations.
  - Develop softwares dealing with modeling/subtracting/masking satellite trails. Develop observation scheduling softwares as well.
  - Do simulations on estimating the impact of satellite trails on specific sciences (e.g., weak lensing signals)

### [射电探测——系外行星搜寻利器 | 赛先生天文](https://mp.weixin.qq.com/s/osmlnGpmrNnCFy2pOQSpZA)

- 射电观测可以用来发现系外行星。系外行星的磁场和host star的磁场发生磁重联时，电子的同步辐射会增强。电子从host star的大气沿着重联的磁力线运动到系外行星需要一些时间，在接收到的信号中也可以看出这个时间差。
- 欧洲的低频阵列射电望远镜（LOFAR）在120-167MHz的频段发现了来自一颗色球层很稳定的M型恒星GJ1151的射电辐射。分析表明，不同于磁场活跃恒星的射电爆发，GJ1151的射电信号不太可能来源于恒星本身，而很可能来源于其周围行星与之相互作用产生的带电粒子的回旋辐射，因此科学家推断在这个恒星周围存在着短周期行星，文章发表在了[Nature Astronomy](https://ui.adsabs.harvard.edu/abs/2020NatAs...4..577V/abstract)上。
- 这种方法拓展了系外行星的搜寻域，因为它不需要“凌星”的configuration。
- FAST的灵敏度集中的频段正好是探测系外行星的窗口。但可能需要扩大FAST的有效口径才行。

### [Unequal effects of the COVID-19 pandemic on scientists](https://www.nature.com/articles/s41562-020-0921-y)

- The working hours of researchers has been decreased after the outbreak of COVID-19.
- Female researches with young dependents are affected most seriously.
- COVID-19 might reform the academia, from the way of working and the hierarchy of researchers.




## March 2020

### [Review: Observational probes of cosmic acceleration](https://ui.adsabs.harvard.edu/abs/2013PhR...530...87W/abstract)

### [Review of Particle Physics](http://pdg.lbl.gov/2019/reviews/contents_sports.html)

- [Dark matter (S. Profumo)](http://pdg.lbl.gov/2019/reviews/rpp2019-rev-dark-matter.pdf)
- [Dark energy (David Weinberg)](http://pdg.lbl.gov/2019/reviews/rpp2019-rev-dark-energy.pdf)
- [CMB (George Smoot)](http://pdg.lbl.gov/2019/reviews/rpp2019-rev-cosmic-microwave-background.pdf)

- [Big Bang cosmology (Peacock)](http://pdg.lbl.gov/2019/reviews/rpp2019-rev-bbang-cosmology.pdf)
- [Cosmological Parameters (Liddle)](http://pdg.lbl.gov/2019/reviews/rpp2019-rev-cosmological-parameters.pdf)

### [Review: Neutrino Mass in Cosmology: Status and Prospects](https://www.annualreviews.org/doi/abs/10.1146/annurev-nucl-102010-130252)

### [Review: Status of neutrino properties and future prospects - Cosmological and astrophysical constraints](https://ui.adsabs.harvard.edu/abs/2017arXiv171207109L/abstract)

### [Early Dark Energy Does Not Restore Cosmological Concordance](https://arxiv.org/abs/2003.07355)

### [Dark Energy Survey Year 1 Results: Cosmological Constraints from Cluster Abundances and Weak Lensing](https://arxiv.org/abs/2002.11124)



## February 2020

### 疫情压力下墙高了，一些解决办法

[macOS - ClashX 使用教程](https://wiki.kache.moe/2019/12/11/macOS-ClashX/)

[V2Ray工具大全](https://v2sx.github.io/V2Ray/)

[Mac下如何科学上网——新方案V2Ray](https://github.com/CocoaKier/MacVPN/wiki/Mac下如何科学上网——新方案V2Ray)

[V2S account](https://v2sx.com/index.php)

### [Point Spread Function Modelling for Wide Field Small Aperture Telescopes with a Denoising Autoencoder](https://arxiv.org/abs/2001.11716)

### [The Three Hundred Project: Backsplash galaxies in simulations of clusters](https://arxiv.org/abs/2001.11518)

## January 2020

### Line-Intensity Mapping Reading List

- [Astro2020 Science White Paper: Astrophysics and Cosmology with Line-Intensity Mapping](https://baas.aas.org/wp-content/uploads/2019/05/101_kovetz.pdf)
- [Line-Intensity Mapping: 2017 Status Report](https://ui.adsabs.harvard.edu/abs/2017arXiv170909066K/abstract)
- [Broadband Intensity Tomography: Spectral Tagging of the Cosmic UV Background](https://iopscience.iop.org/article/10.3847/1538-4357/ab1b35)
- [Measuring the EoR Power Spectrum without Measuring the EoR Power Spectrum](https://iopscience.iop.org/article/10.3847/1538-4357/ab0a08/pdf)


### [Adam Riess: The expansion of the Universe is faster than expected](https://www.nature.com/articles/s42254-019-0137-0#change-history)

- H0LiCOW is independent of Cepheids and SNe Ia, but also yields ~70 km/s/Mpc. 
- $$\Lambda CDM$$: six parameters with well-tested ansatzes. Dark energy with an equation of state lower than vacuum energy could produce stronger acceleration and explain the discrepancy, but this possibility is disfavoured by other intermediate­ redshift measurements.
- Early Dark Energy (EDE) (injected shortly before CMB) could have increased the early expansion, decreased the sound horizon, and raise the predicted $$H_0$$. But the value of EDE must be fine-tuned. Adding new particles (such as neutrino) could play the same role, but will create conflicts with CMB. 

### [The Hubble Hunter's Guide](https://ui.adsabs.harvard.edu/abs/2019arXiv190803663K/abstract)

### [Planck evidence for a closed Universe and a possible crisis for cosmology](https://www.nature.com/articles/s41550-019-0906-9)

### [Vogelsberger: Cosmological simulations of galaxy formation](https://www.nature.com/articles/s42254-019-0127-2)

### [A parallax distance to 3C 273 through spectroastrometry and reverberation mapping](https://www.nature.com/articles/s41550-019-0979-5)

- New component in the cosmic distance ladder! Using GRAVITY spectroastrometry (SA) (resolution ~10 mas), they measured the angular size of the broad-line-region to be 46 mas. They also used reverberation mapping (RM) to measure the physical size of the broad-line-region. Thus a distance is derived and Hubble constant is measured as $$H_0 = 71.5^{+11.9}_{-10.6}\ \mathrm{km/s/Mpc}$$. With larger sample size, the Hubble constant can be measured to 3% precision, and help test the Hubble tension.  SARM could simultaneously measure the SMBH mass (Doppler broaden + physics size).

### [Iterative Emulation is the Sincerest Form of Parameter Estimation](https://astrobites.org/2020/01/13/iterative-emulation-is-the-sincerest-form-of-parameter-estimation/)

Paper link: [Cosmological parameter estimation via iterative emulation of likelihoods](https://arxiv.org/abs/1912.08806). See also [Your Gateway to the Bayesian Realm](https://astrobites.org/2011/11/26/your-gateway-to-the-bayesian-realm/); [A Zero-Math Introduction to Markov Chain Monte Carlo Methods](https://towardsdatascience.com/a-zero-math-introduction-to-markov-chain-monte-carlo-methods-dcba889e0c50).

### [Alex Drlica-Wagner: Probing the Fundamental Nature of Dark Matter with the Large Synoptic Survey Telescope](https://ui.adsabs.harvard.edu/abs/2019arXiv190201055D/abstract)

### [Early-type Host Galaxies of Type Ia Supernovae. II. Evidence for Luminosity Evolution in Supernova Cosmology](https://ui.adsabs.harvard.edu/abs/2019arXiv191204903K/abstract)

{% include image.html url="/images/astro-ph/Kang2019.png" caption="Luminosity evolution of SNe Ia can mimic dark energy" width=350 align="right" %}

- The Supernovae Cosmology (Riess, Perlmutter, Schmidt) has a fundamental assumption: the absolute magnitude of SNe Ia after standardization doesn't evolve with time. They compared nearby SNe Ia in late- and early-type galaxies, concluded no significant differences. However, recent observations have found the dependence of standardized luminosity on other factors: SN Ia is 1) brighter in ETG; 2) brighter in more massive galaxies; 3) fainter in high SFR region. However, how does these factors (SFR, mass, morphology) directly affect the progenitor of SNe Ia? We don't know.
- The authors proposed that **stellar age** and **metallicity** are the direct factors of the luminosity variation. The dependence on mass, SFR, morphology can be attributed to age and metallicity. Hence they studied ~50 ETGs, derived their stellar ages and metallicities, concluded that **SNe Ia in galaxies with older stellar population is brighter**. The correlation is significant. 

- Why do we need dark energy? Because we have found that the luminosity of SNe Ia at high-z is fainter than expected, so we need dark energy to dilute the light and increase the distance. However, if SN Ia in young ETG (which is at high-z) is intrinsically fainter, then we don't need dark energy any more. In Figure 16, they showed that the (reported) luminosity evolution of SNe Ia can mimic dark energy. 
