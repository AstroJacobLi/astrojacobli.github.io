<!DOCTYPE html>
<html>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { extensions: ["AMSmath.js", "AMSsymbols.js"] }});
  MathJax.Hub.Config({
  TeX: {
    Macros: {
      md: "{\\text{d}}",
      dbar: "{\\text{d}}",
      bold: ["{\\bf #1}",1],
      Abs: ['\\left\\lvert #2 \\right\\rvert_{\\text{#1}}', 2, ""]
    }
  }
});
</script>
  <head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-106001791-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-106001791-2');
  </script>
  
  <meta name="google-site-verification" content="dW5mXMbkY4MK952APZMO3_VkJwk9q6Rv1O8KgPZVidE" />
  <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { extensions: ["AMSmath.js", "AMSsymbols.js"] }});
  MathJax.Hub.Config({
  TeX: {
    Macros: {
      md: "{\\text{d}}",
      dbar: "{\\text{d}}",
      bold: ["{\\bf #1}",1],
      Abs: ['\\left\\lvert #2 \\right\\rvert_{\\text{#1}}', 2, ""]
    	}
  	}
	});
	</script>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Scikit-learn</title>
  <meta name="description" content="Quick StartMachine learning  Supervised learning: Classification, Regression. (Learn the map \(X\to y\))  Unsupervised learning: Clustering, density estimati...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://localhost:4000/blog/2018/12/Scikit-learn">
  <link rel="alternate" type="application/rss+xml" title="Jiaxuan Li 李嘉轩" href="http://localhost:4000/feed.xml" />
<link rel='stylesheet' id='open-sans-css'  href='//fonts.googleapis.com/css?family=Open+Sans%3A300italic%2C400italic%2C600italic%2C300%2C400%2C600&#038;subset=latin%2Clatin-ext&#038;ver=4.2.4' type='text/css' media='all' />
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:600italic,600,400,400italic' rel='stylesheet' type='text/css'>



<link rel="apple-touch-icon-precomposed" sizes="57x57" href="./images/favicon/apple-touch-icon-57x57.png" />
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="./images/favicon/apple-touch-icon-114x114.png" />
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="./images/favicon/apple-touch-icon-72x72.png" />
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="./images/favicon/apple-touch-icon-144x144.png" />
<link rel="apple-touch-icon-precomposed" sizes="60x60" href="./images/favicon/apple-touch-icon-60x60.png" />
<link rel="apple-touch-icon-precomposed" sizes="120x120" href="./images/favicon/apple-touch-icon-120x120.png" />
<link rel="apple-touch-icon-precomposed" sizes="76x76" href="./images/favicon/apple-touch-icon-76x76.png" />
<link rel="apple-touch-icon-precomposed" sizes="152x152" href="./images/favicon/apple-touch-icon-152x152.png" />
<link rel="icon" type="image/png" href="./images/favicon/favicon-196x196.png" sizes="196x196" />
<link rel="icon" type="image/png" href="./images/favicon/favicon-96x96.png" sizes="96x96" />
<link rel="icon" type="image/png" href="./images/favicon/favicon-32x32.png" sizes="32x32" />
<link rel="icon" type="image/png" href="./images/favicon/favicon-16x16.png" sizes="16x16" />
<link rel="icon" type="image/png" href="./images/favicon/favicon-128.png" sizes="128x128" />
<meta name="application-name" content="&nbsp;"/>
<meta name="msapplication-TileColor" content="#FFFFFF" />
<meta name="msapplication-TileImage" content="./images/favicon/mstile-144x144.png" />
<meta name="msapplication-square70x70logo" content="./images/favicon/mstile-70x70.png" />
<meta name="msapplication-square150x150logo" content="./images/favicon/mstile-150x150.png" />
<meta name="msapplication-wide310x150logo" content="./images/favicon/mstile-310x150.png" />
<meta name="msapplication-square310x310logo" content="./images/favicon/mstile-310x310.png" />





</head>

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Scikit-learn | Jiaxuan Li 李嘉轩</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Scikit-learn" />
<meta name="author" content="Jiaxuan Li" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Quick Start Machine learning Supervised learning: Classification, Regression. (Learn the map \(X\to y\)) Unsupervised learning: Clustering, density estimation. Dimensionality reduction. Scikit-learn Cheat Sheet Scikit-learn Data is constructed by the format: (n_samples, n_features) Datasets Examples: iris, diabetes, digits, etc. Every estimator class has .fit(X_train, y_train) and .predict(X_test, y_test). Export the estimator into a file: from joblib import jump, load dump(clf, &#39;filename.joblib&#39;) clf = load(&#39;filename.joblib&#39;) You can change the parameters of an estimator after construct it by using .set_params() method. In classification, y can have multi-labels (like in iris). You can use sklearn.preprocessing.LabelBinarizer or sklearn.preprocessing.MultiLabelBinarizer, and classify using sklearn.multiclass.OneVsRestClassifier. Supervised learning: predicting an output variable from high-dimensional observations Nearest Neighbor Classification kNN (k-th nearest neighbor classification): sklearn.neighbors.KNeighborsClassifier is also an estimator. Curse of dimensionality: if your sample is small and dimension is high, then your sample will be very sparse. Also it’s very interesting that in high dimensions, the high-dimensional unit hypercube can be said to consist almost entirely of the “corners” of the hypercube, with almost no “middle”. Linear model: Regression Linear models: \(y = X \beta + \epsilon\), in which \(y\) and \(\epsilon\) are \((n\times 1)\), \(X\) is \((n\times m)\) and \(\beta\) is \((m\times 1)\). Here \(X\) is data, \(y\) is target, \(\beta\) is coefficients and \(\epsilon\) is observation noise. Linear Regression estimator: sklearn.linear_model.LinearRegression. Shrinkage: sklearn.linear_model.Ridge and sklearn.linear_model.Lasso. If we use linear model to do classification, remember using ‘logistic’ or ‘sigmoid’ function: sklearn.linear_model.LogisticRegression. Support Vector Machines (SVMs) SVMs has regularization parameter \(C\), smaller \(C\) means more regularization. sklearn.svm.SVC is the estimator. Different kernels give different results. Model selection and cross-validation Since an estimator has some parameters, we need to choose the best values for these parameters to get the best behavior. So we use score and ‘cross-validation’. CV generators: sklearn.model_selection.KFold can be used as kfold = KFold(n_splits=5), for train, test in kfold.split(X):. This can generate 5 groups of training samples and test samples. Then we can calculate scores for each set. Another way is sklearn.model_selection.cross_val_score(svc, X, y, cv=5, n_jobs=-1). Another way is grid-search the parameter space using sklearn.model_selection.GridSearchCV(svc, param_grid=dict(C=Cs)). Some estimator can automatically give the best values of some parameters. For example constructing an estimator estimator = sklearn.linear_model.LassoCV(cv=5) and fit it with data. Then we can retrieve the best parameter ‘alpha’ by estimator.alpha_. But we should note, this may not be reliable. We’d better estimate the best parameter by ourselves. pca.fit_transform = first fit, then transform." />
<meta property="og:description" content="Quick Start Machine learning Supervised learning: Classification, Regression. (Learn the map \(X\to y\)) Unsupervised learning: Clustering, density estimation. Dimensionality reduction. Scikit-learn Cheat Sheet Scikit-learn Data is constructed by the format: (n_samples, n_features) Datasets Examples: iris, diabetes, digits, etc. Every estimator class has .fit(X_train, y_train) and .predict(X_test, y_test). Export the estimator into a file: from joblib import jump, load dump(clf, &#39;filename.joblib&#39;) clf = load(&#39;filename.joblib&#39;) You can change the parameters of an estimator after construct it by using .set_params() method. In classification, y can have multi-labels (like in iris). You can use sklearn.preprocessing.LabelBinarizer or sklearn.preprocessing.MultiLabelBinarizer, and classify using sklearn.multiclass.OneVsRestClassifier. Supervised learning: predicting an output variable from high-dimensional observations Nearest Neighbor Classification kNN (k-th nearest neighbor classification): sklearn.neighbors.KNeighborsClassifier is also an estimator. Curse of dimensionality: if your sample is small and dimension is high, then your sample will be very sparse. Also it’s very interesting that in high dimensions, the high-dimensional unit hypercube can be said to consist almost entirely of the “corners” of the hypercube, with almost no “middle”. Linear model: Regression Linear models: \(y = X \beta + \epsilon\), in which \(y\) and \(\epsilon\) are \((n\times 1)\), \(X\) is \((n\times m)\) and \(\beta\) is \((m\times 1)\). Here \(X\) is data, \(y\) is target, \(\beta\) is coefficients and \(\epsilon\) is observation noise. Linear Regression estimator: sklearn.linear_model.LinearRegression. Shrinkage: sklearn.linear_model.Ridge and sklearn.linear_model.Lasso. If we use linear model to do classification, remember using ‘logistic’ or ‘sigmoid’ function: sklearn.linear_model.LogisticRegression. Support Vector Machines (SVMs) SVMs has regularization parameter \(C\), smaller \(C\) means more regularization. sklearn.svm.SVC is the estimator. Different kernels give different results. Model selection and cross-validation Since an estimator has some parameters, we need to choose the best values for these parameters to get the best behavior. So we use score and ‘cross-validation’. CV generators: sklearn.model_selection.KFold can be used as kfold = KFold(n_splits=5), for train, test in kfold.split(X):. This can generate 5 groups of training samples and test samples. Then we can calculate scores for each set. Another way is sklearn.model_selection.cross_val_score(svc, X, y, cv=5, n_jobs=-1). Another way is grid-search the parameter space using sklearn.model_selection.GridSearchCV(svc, param_grid=dict(C=Cs)). Some estimator can automatically give the best values of some parameters. For example constructing an estimator estimator = sklearn.linear_model.LassoCV(cv=5) and fit it with data. Then we can retrieve the best parameter ‘alpha’ by estimator.alpha_. But we should note, this may not be reliable. We’d better estimate the best parameter by ourselves. pca.fit_transform = first fit, then transform." />
<link rel="canonical" href="http://localhost:4000/blog/2018/12/Scikit-learn" />
<meta property="og:url" content="http://localhost:4000/blog/2018/12/Scikit-learn" />
<meta property="og:site_name" content="Jiaxuan Li 李嘉轩" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-12-10T05:18:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Scikit-learn" />
<meta name="twitter:site" content="@AstroJacobLi" />
<meta name="twitter:creator" content="@Jiaxuan Li" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Jiaxuan Li"},"dateModified":"2018-12-10T05:18:00-05:00","datePublished":"2018-12-10T05:18:00-05:00","description":"Quick Start Machine learning Supervised learning: Classification, Regression. (Learn the map \\(X\\to y\\)) Unsupervised learning: Clustering, density estimation. Dimensionality reduction. Scikit-learn Cheat Sheet Scikit-learn Data is constructed by the format: (n_samples, n_features) Datasets Examples: iris, diabetes, digits, etc. Every estimator class has .fit(X_train, y_train) and .predict(X_test, y_test). Export the estimator into a file: from joblib import jump, load dump(clf, &#39;filename.joblib&#39;) clf = load(&#39;filename.joblib&#39;) You can change the parameters of an estimator after construct it by using .set_params() method. In classification, y can have multi-labels (like in iris). You can use sklearn.preprocessing.LabelBinarizer or sklearn.preprocessing.MultiLabelBinarizer, and classify using sklearn.multiclass.OneVsRestClassifier. Supervised learning: predicting an output variable from high-dimensional observations Nearest Neighbor Classification kNN (k-th nearest neighbor classification): sklearn.neighbors.KNeighborsClassifier is also an estimator. Curse of dimensionality: if your sample is small and dimension is high, then your sample will be very sparse. Also it’s very interesting that in high dimensions, the high-dimensional unit hypercube can be said to consist almost entirely of the “corners” of the hypercube, with almost no “middle”. Linear model: Regression Linear models: \\(y = X \\beta + \\epsilon\\), in which \\(y\\) and \\(\\epsilon\\) are \\((n\\times 1)\\), \\(X\\) is \\((n\\times m)\\) and \\(\\beta\\) is \\((m\\times 1)\\). Here \\(X\\) is data, \\(y\\) is target, \\(\\beta\\) is coefficients and \\(\\epsilon\\) is observation noise. Linear Regression estimator: sklearn.linear_model.LinearRegression. Shrinkage: sklearn.linear_model.Ridge and sklearn.linear_model.Lasso. If we use linear model to do classification, remember using ‘logistic’ or ‘sigmoid’ function: sklearn.linear_model.LogisticRegression. Support Vector Machines (SVMs) SVMs has regularization parameter \\(C\\), smaller \\(C\\) means more regularization. sklearn.svm.SVC is the estimator. Different kernels give different results. Model selection and cross-validation Since an estimator has some parameters, we need to choose the best values for these parameters to get the best behavior. So we use score and ‘cross-validation’. CV generators: sklearn.model_selection.KFold can be used as kfold = KFold(n_splits=5), for train, test in kfold.split(X):. This can generate 5 groups of training samples and test samples. Then we can calculate scores for each set. Another way is sklearn.model_selection.cross_val_score(svc, X, y, cv=5, n_jobs=-1). Another way is grid-search the parameter space using sklearn.model_selection.GridSearchCV(svc, param_grid=dict(C=Cs)). Some estimator can automatically give the best values of some parameters. For example constructing an estimator estimator = sklearn.linear_model.LassoCV(cv=5) and fit it with data. Then we can retrieve the best parameter ‘alpha’ by estimator.alpha_. But we should note, this may not be reliable. We’d better estimate the best parameter by ourselves. pca.fit_transform = first fit, then transform.","headline":"Scikit-learn","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/2018/12/Scikit-learn"},"url":"http://localhost:4000/blog/2018/12/Scikit-learn"}</script>
<!-- End Jekyll SEO tag -->

  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Jiaxuan Li 李嘉轩</a>


    <nav class="site-nav">

      <a href="#" class="menu-icon menu.open">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>  

    <div class="trigger"><h1>Main Navigation</h1>

 <ul class="menu">

    
    
     <li><a href="/about/" class="page-link">About</a>
    
    </li>
    
    
     <li><a href="/cv/" class="page-link">CV</a>
    
    </li>
    
    
     <li><a href="/research/" class="page-link">Research</a>
    
    </li>
    
    
     <li><a href="/blog/" class="page-link">Blog</a>
    
    </li>
    
    
     <li><a href="/portfolio/" class="page-link">Gallery</a>
    
    </li>
    
    
    <li><a href="/miscellany/" class="page-link">Miscellany</a>
    <ul class="sub-menu">
    
    <li><a href="/miscellany/photography">Photography</a></li>
    
    <li><a href="/miscellany/cooking/">Cooking</a></li>
    
    <li><a href="/miscellany/links">Links</a></li>
    
    </ul>
    
    </li>
    
    </ul>


     </div>  
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">
  <header class="post-header">
    <h1 class="post-title">Scikit-learn</h1>
    <p class="post-meta">Posted on December 10, 2018 by  Jiaxuan Li  

  in
  
  <a href="/categories/#coding" title="coding">coding</a>&nbsp;
  


</p>
  </header>

  <article class="post-content">
    <h1 id="quick-start">Quick Start</h1>

<h2 id="machine-learning">Machine learning</h2>
<ul>
  <li>Supervised learning: Classification, Regression. (Learn the map \(X\to y\))</li>
  <li>Unsupervised learning: Clustering, density estimation.</li>
  <li>Dimensionality reduction.</li>
</ul>
<dl class="wp-caption alignright" style="width: 500px">

<dt><a href=""><img class="" src="/images/ml_map.png" alt="Scikit-learn Cheat Sheet" /></a></dt>

<dd>Scikit-learn Cheat Sheet</dd>
</dl>

<h2 id="scikit-learn">Scikit-learn</h2>
<ul>
  <li>Data is constructed by the format: (n_samples, n_features)</li>
  <li>Datasets Examples: iris, diabetes, digits, etc.</li>
  <li>Every <code class="language-plaintext highlighter-rouge">estimator</code> class has <code class="language-plaintext highlighter-rouge">.fit(X_train, y_train)</code> and <code class="language-plaintext highlighter-rouge">.predict(X_test, y_test)</code>.</li>
  <li>Export the <code class="language-plaintext highlighter-rouge">estimator</code> into a file:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">jump</span><span class="p">,</span> <span class="n">load</span>
<span class="n">dump</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="s">'filename.joblib'</span><span class="p">)</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s">'filename.joblib'</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>You can change the parameters of an <code class="language-plaintext highlighter-rouge">estimator</code> after construct it by using <code class="language-plaintext highlighter-rouge">.set_params()</code> method.</li>
  <li>In classification, <code class="language-plaintext highlighter-rouge">y</code> can have multi-labels (like in iris). You can use <code class="language-plaintext highlighter-rouge">sklearn.preprocessing.LabelBinarizer</code> or <code class="language-plaintext highlighter-rouge">sklearn.preprocessing.MultiLabelBinarizer</code>, and classify using <code class="language-plaintext highlighter-rouge">sklearn.multiclass.OneVsRestClassifier</code>.</li>
</ul>

<h1 id="supervised-learning-predicting-an-output-variable-from-high-dimensional-observations"><a href="https://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html">Supervised learning: predicting an output variable from high-dimensional observations</a></h1>

<h2 id="nearest-neighbor-classification">Nearest Neighbor Classification</h2>
<ul>
  <li>kNN (k-th nearest neighbor classification): <code class="language-plaintext highlighter-rouge">sklearn.neighbors.KNeighborsClassifier</code> is also an estimator.</li>
  <li><a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">Curse of dimensionality</a>: if your sample is small and dimension is high, then your sample will be very sparse. Also it’s very interesting that in high dimensions, the high-dimensional unit hypercube can be said to consist almost entirely of the “corners” of the hypercube, with almost no “middle”.</li>
</ul>

<h2 id="linear-model-regression">Linear model: Regression</h2>
<ul>
  <li>Linear models: \(y = X \beta + \epsilon\), in which \(y\) and \(\epsilon\) are \((n\times 1)\), \(X\) is \((n\times m)\) and \(\beta\) is \((m\times 1)\). Here \(X\) is data, \(y\) is target, \(\beta\) is coefficients and \(\epsilon\) is observation noise.</li>
  <li>Linear Regression estimator: <code class="language-plaintext highlighter-rouge">sklearn.linear_model.LinearRegression</code>.</li>
  <li>Shrinkage: <code class="language-plaintext highlighter-rouge">sklearn.linear_model.Ridge</code> and <code class="language-plaintext highlighter-rouge">sklearn.linear_model.Lasso</code>.</li>
  <li>If we use linear model to do classification, remember using ‘logistic’ or ‘sigmoid’ function: <code class="language-plaintext highlighter-rouge">sklearn.linear_model.LogisticRegression</code>.</li>
</ul>

<h2 id="support-vector-machines-svms">Support Vector Machines (SVMs)</h2>
<ul>
  <li>SVMs has regularization parameter \(C\), smaller \(C\) means more regularization. <code class="language-plaintext highlighter-rouge">sklearn.svm.SVC</code> is the estimator.</li>
  <li>Different kernels give different results.</li>
</ul>

<h1 id="model-selection-and-cross-validation"><a href="https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html">Model selection and cross-validation</a></h1>
<p>Since an estimator has some parameters, we need to choose the best values for these parameters to get the best behavior. So we use <code class="language-plaintext highlighter-rouge">score</code> and ‘cross-validation’.</p>
<ul>
  <li>CV generators: <code class="language-plaintext highlighter-rouge">sklearn.model_selection.KFold</code> can be used as <code class="language-plaintext highlighter-rouge">kfold = KFold(n_splits=5)</code>, <code class="language-plaintext highlighter-rouge">for train, test in kfold.split(X):</code>. This can generate 5 groups of training samples and test samples. Then we can calculate scores for each set.</li>
  <li>Another way is <code class="language-plaintext highlighter-rouge">sklearn.model_selection.cross_val_score(svc, X, y, cv=5, n_jobs=-1)</code>.</li>
  <li>Another way is grid-search the parameter space using <code class="language-plaintext highlighter-rouge">sklearn.model_selection.GridSearchCV(svc, param_grid=dict(C=Cs))</code>.</li>
  <li>Some estimator can automatically give the best values of some parameters. For example constructing an estimator <code class="language-plaintext highlighter-rouge">estimator = sklearn.linear_model.LassoCV(cv=5)</code> and fit it with data. Then we can retrieve the best parameter ‘alpha’ by <code class="language-plaintext highlighter-rouge">estimator.alpha_</code>. But we should note, this may not be reliable. We’d better estimate the best parameter by ourselves.</li>
</ul>

<p><code class="language-plaintext highlighter-rouge">pca.fit_transform</code> = first <code class="language-plaintext highlighter-rouge">fit</code>, then <code class="language-plaintext highlighter-rouge">transform</code>.</p>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

<!--     <h2 class="footer-heading">Jiaxuan Li 李嘉轩</h2> -->

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-2">
        <br>
        <ul class="contact-list">
          <li><strong>Jiaxuan Li 李嘉轩</strong></li>
          <li>Graduate Student</li>
          <li><a href="mailto:jiaxuanl@princeton.edu">jiaxuanl@princeton.edu</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <br>
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/AstroJacobLi">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>

              <span class="username">AstroJacobLi</span>
            </a>
          </li>
          

          
          <li>
            <a href="https://twitter.com/AstroJacobLi">
              <span class="icon  icon--twitter">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                  c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
                </svg>
              </span>

              <span class="username">AstroJacobLi</span>
            </a>
          </li>
          
          
          <li>
            <a href="https://orcid.org"><img alt="ORCID logo" src="https://orcid.org/sites/default/files/images/orcid_16x16.png" width="16" height="16" hspace="0" /></a> <a href="https://orcid.org/0000-0001-9592-4190"> <font size="-1">0000-0001-9592-4190</font></a>
          </li>

          

          
        </ul>
      </div>

      <div class="footer-col  footer-col-5">
        <br>
         <p class="text">
          Department of Astrophysical Sciences<br>
          012 Peyton Hall, 4 Ivy Lane,<br>
          Princeton, NJ 08544, USA<br>
      </div>

      <div class="footer-col  footer-col-2">
         <script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5umlf3qf2sp&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>
      </div>

    </div>

  </div>

</footer>

  </body>

</html>
<!-- d.s.m.s.050600.062508.030515.080516.030818 | "Baby, I'm Yours" -->