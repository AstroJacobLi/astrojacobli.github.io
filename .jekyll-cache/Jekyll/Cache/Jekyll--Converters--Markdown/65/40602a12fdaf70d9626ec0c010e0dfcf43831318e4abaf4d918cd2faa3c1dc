I"F&<h1 id="心理统计">心理统计</h1>

<h3 id="power-analysis">Power analysis</h3>

<p>Power = $1-\beta$. 通过power analysis来限制样本量：
\(n = \frac{(z_{1-\beta} + z_{1-\alpha})^2\sigma^2}{(\mu_0 - \mu_1)^2}\)
一般研究中 $\alpha = 0.05,\ \beta = 0.80$</p>

<h3 id="estimation">Estimation</h3>

<p>Estimation: 利用样本统计量 ($\overline{M},\ s$) 推断总体的参数 ($\mu,\ \sigma$)。Estimation 和 假设检验是互补的。</p>

<p>对于一个实验，先做假设检验来判断某种treatment是否有用，再利用estimation来估计这种treatment有多有用。</p>

<p>在estimation中, $\dfrac{\overline{X} - \mu}{s/\sqrt{n}}$ 不是正态分布，而是student-t分布，所以这里应该用 $t_{\alpha/2}$, 而非  $Z_{\alpha/2}$. Student-t分布针对总体标准差未知的时候。t分布实际上是对样本std自由度的一个改正。</p>

<h3 id="t-test">t-test</h3>
<p>对总体的分布没有先验知识的时候，Z分布就不行了。总之，如果对总体没什么了解的话，就用t-test来检验均值。</p>

<h1 id="普通统计学">普通统计学</h1>

<h2 id="第一章">第一章</h2>

<p>调查的两种方法：实验和观察。观察到的信息无法做casual的判断，只能给出correlation。但是实验室做实验可以做出因果关系来。天文观测没法给出casaul link，这需要注意。天文观测没法控制变量。我们只能知道结果，但是可以通过贝叶斯方法来推断是什么原因产生了这些结果（通过一定的概率模型）。</p>

<p>普查有time-delay。给定一个概率抽样，那么就可以算出抽样误差。</p>

<p>整群抽样能够减小调查的成本，但是需要注意群与群之间差异要较小。</p>

<p>非概率抽样很难做统计推断，很难推广到总体。</p>

<p>一个个体被调查到的概率是\(p\)， 则其代表性是\(1/p\).</p>

<p>SImpson悖论：如果忘记了一些混淆因素，那么结论很可能完全不可信。</p>

<p>如果是概率抽样，那么抽样误差可以算出来。在\(N\)个样本中抽样，那么一个标准差一定小于\(1/\sqrt{N}\).</p>

<p>未响应样本：给人打电话有人不接。这样可以算出来总统支持率的上下限。</p>

<p>数据挖掘里的EM算法（Expectation &amp; Maximization）：通过迭代，把未响应数据填入列联表。这个算法成立要求“随机不相应”。</p>

<p>响应误差：注意措辞。调查时有可能强加给人一个观点，这对中立的人/不了解情况的人不公平。访员对结果也有影响。</p>

<p><strong>试验是检验变量间因果关系的一种方法。</strong>做调查没法得到因果关系，只能得到相关信息。相关关系可以做预测，但是没法做决策。给定一个巧克力消耗量，我可以预测诺贝尔奖量。但是没法让说让一个国家猛吃巧克力，然后这个国家诺贝尔奖量就猛的上升了。决策靠的是因果，预测靠的是相关。在做实验时，一定要控制其他变量相同（或者完全随机，随机选择实验对象），然后控制一个变量不同。要考虑到各种因素，不要带来Simpson悖论。</p>

<p><strong>随机化实验方法：</strong>随机选取病人，从而各种因素在两组内分布都是相近的。随机化对照是<strong>金标准</strong>。随机对照和历史对照可以完全不一样，衡量药品的药效一定要做<strong>随机化对照</strong>。对人做实验是有ethical committee来判断是否合乎伦理。</p>

<table>
  <tbody>
    <tr>
      <td><a href="https://en.wikipedia.org/wiki/Median_absolute_deviation">Median absolute deviation</a>: $$\text{MAD} = \text{median}(</td>
      <td>X_i - \text{median}(X)</td>
      <td>)$$.</td>
    </tr>
  </tbody>
</table>

<h1 id="gaussian-process">Gaussian Process</h1>

<p><a href="http://www.gaussianprocess.org/gpml/chapters/RW.pdf">Gaussian Process for Machine Learning</a></p>

<h1 id="david-hoggs-paper-on-statistics">David Hogg’s paper on Statistics</h1>

<h3 id="data-analysis-recipes-fitting-a-model-to-data"><a href="https://arxiv.org/abs/1008.4686">Data analysis recipes: Fitting a model to data</a></h3>
<h3 id="data-analysis-recipes-probability-calculus-for-inference"><a href="https://arxiv.org/abs/1205.4446v1">Data analysis recipes: Probability calculus for inference</a></h3>

<hr />

<h1 id="plot-with-python"><a href="https://github.com/AstroJacobLi/astro-ph/wiki/Plot-with-Python">Plot with Python</a></h1>

<h1 id="using-python-to-do-data-analysis">Using Python to do Data Analysis</h1>
<ul>
  <li>Very good tutorial of using python and Jupiter to analyze data, by Jake VanderPlas: <a href="http://jakevdp.github.io/blog/2017/03/03/reproducible-data-analysis-in-jupyter/">Reproducible Data Analysis in Jupyter</a></li>
  <li>Various ways to <a href="https://realpython.com/python-histograms/#a-fancy-alternative-with-seaborn">draw a histogram</a>, with NumPy, Matplotlib, Pandas and seaborn, by Brad Solomon.</li>
  <li><a href="https://www.youtube.com/watch?v=DifMYH3iuFw">Introduction of PyData community, by Jake VanderPlas</a></li>
</ul>

<h1 id="bayesian-statistics">Bayesian Statistics</h1>
<h2 id="frequentism-vs-bayesianism-by-jake-vanderplas"><a href="https://www.youtube.com/watch?v=KhAUfqhLakw">Frequentism v.s. Bayesianism, by Jake VanderPlas</a></h2>
<h2 id="akaike-information-criterion"><a href="https://en.wikipedia.org/wiki/Akaike_information_criterion">Akaike Information Criterion</a></h2>
<h2 id="emcee-seriously-kick-ass-mcmc-tool"><code class="language-plaintext highlighter-rouge">emcee</code>: Seriously Kick-Ass MCMC tool</h2>
<ul>
  <li><code class="language-plaintext highlighter-rouge">emcee</code> is a python module that implements a very cool MCMC sampling algorithm cample an ensemble sampler. In order to more efficiently sample the parameter space, many samplers (called walkers) run in parallel and periodically exchange states. emcee is available from this website:
http://dan.iel.fm/emcee/current/. And some examples for EMCEE: http://dfm.io/emcee/current/user/line/</li>
  <li>The most up-to-date documents of <code class="language-plaintext highlighter-rouge">emcee</code> is https://emcee.readthedocs.io/en/latest/tutorials/line/. It’s much prettier and understandable than before. You can install newest version by clone its GitHub, then <code class="language-plaintext highlighter-rouge">pip uninstall emcee</code>, and using <code class="language-plaintext highlighter-rouge">python setup.py install</code> to install new version.</li>
  <li>If you google <code class="language-plaintext highlighter-rouge">emcee example</code>, you can already find a lot of good tutorials and examples.  But if you need an example for complex astrophysical application, my personal recommendation is the <code class="language-plaintext highlighter-rouge">prospector</code> SED fitting code by Ben Johnson:  https://github.com/bd-j/prospector</li>
  <li><code class="language-plaintext highlighter-rouge">emcee</code> employs <em>Affine Invariant Markov chain Monte Carlo (MCMC) Ensemble sampler.</em> But Metropolis-Hastings sampler and The Parallel-Tempered Ensemble Sampler (PTMCMC) can also be found in <code class="language-plaintext highlighter-rouge">emcee</code>. The PTMCMC is useful if you expect your distribution to be multi-modal.</li>
</ul>

<blockquote>
  <p>Related to today’s discussion on MCMC:</p>

  <ol>
    <li>About autocorrelation time in emcee: https://emcee.readthedocs.io/en/latest/tutorials/autocorr/ In <code class="language-plaintext highlighter-rouge">emcee</code> the <code class="language-plaintext highlighter-rouge">autocorr.py</code> deals with this, and more here on how to use it to check convergence in real application: https://emcee.readthedocs.io/en/latest/tutorials/monitor/</li>
    <li>The <code class="language-plaintext highlighter-rouge">dynesty</code> Dynamic Nested Sampling code is here: https://dynesty.readthedocs.io/en/latest/ ; <code class="language-plaintext highlighter-rouge">prospector</code> has an example of its application here: https://github.com/bd-j/prospector/blob/master/prospect/fitting/nested.py</li>
    <li>About the <code class="language-plaintext highlighter-rouge">pickle</code> issue, the general description of “pickleable” objects can be found here: https://docs.python.org/3/library/pickle.html#pickle-picklable  Although in real application, this can get tricky.</li>
    <li>About the time spent on each realization of the likelihood, if you think there might be room for improvement, I always use <code class="language-plaintext highlighter-rouge">cProfile</code> to profile the time spent on each functional call.  It is very easy to use.</li>
  </ol>
</blockquote>

<hr />

<ul>
  <li>
    <p><a href="http://adsabs.harvard.edu/abs/2005AJ....129.1706F">Ford 2005</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/astro-ph/0512634">Ford 2005</a></p>
  </li>
  <li>
    <p>A mcmc tool developed by astronomers: <a href="http://dfm.io/emcee/current/user/quickstart/">emcee</a></p>
  </li>
  <li>
    <p>Astrobite on <a href="https://astrobites.org/2011/07/26/astrostatistics-how-to-fit-a-model-to-data/">fitting (Hogg paper)</a></p>
  </li>
  <li>
    <p>Astrobite on correlated <a href="https://astrobites.org/2014/07/01/beyond-chi-squared-an-introduction-to-correlated-noise/">errors</a></p>
  </li>
  <li>
    <p><a href="http://www.gaussianprocess.org">Gaussian Process</a></p>
  </li>
  <li>
    <p><a href="https://healthyalgorithms.com/tag/pymc/">pymc tutorials</a></p>
  </li>
  <li>
    <p><a href="http://jakevdp.github.io/blog/2017/03/03/reproducible-data-analysis-in-jupyter/">Data analysis using Jupyter</a></p>
  </li>
</ul>
:ET